\documentclass{llncs}
% * <peter.christen@anu.edu.au> 2016-08-19T15:07:29.567Z:
%
% ^.
%\usepackage{llncsdoc}
\usepackage{epsfig}
\usepackage{graphicx}

%\usepackage{caption}
%\usepackage{subcaption}

\newtheorem{mydef}{Def.}
\newtheorem{myhyp}{Hypothesis}

\usepackage{url}
\usepackage{hyperref}

\pagestyle{empty}

% PAKDD 2018: maximum 12 pages
% Abstract max 200 words
% title, abstract: 1/2 page
% introduction: 1 1/2 page (2 pages so far)
% related work: 1 to 1 1/2 pages (3 pages so far)
% method: 4 pages (7 pages so far)
% experiments: 3 (10 pages so far)
% discussion /conclusion: 1 page
% citations 1

% ====================================================================

% Eamonn ICDM'10 tutorial slides
% - clear problem statement in abstract
% - To convince a reviewer, you must think like a reviewer

% Writing the paper:
% - Make a working title
% - Introduce the topic and define (informally at this stage)
%   terminology
% - Motivation: Emphasize why is the topic important
% - Relate to current knowledge: what’s been done
% - Indicate the gap: what need’s to be done?
% - Formally pose research questions
% - Explain any necessary background material.
% - Introduce formal definitions.
% - Introduce your novel algorithm/representation/data structure etc.
% - Describe experimental set-up, explain what the experiments will
%   show
% - Describe the datasets
% - Summarize results with figures/tables
% - Discuss results
% - Explain conflicting results, unexpected findings and discrepancies
%   with other research
% - State limitations of the study
% - State importance of findings
% - Announce directions for further research
% - Acknowledgements
% - References
%
% - Don’t make the reviewer of your paper think!
% - Reviewers make an initial impression on the first page and don’t
%   change 80% of the time
% - A good introduction with a good motivation is half your success
% - By the end of the introduction the reviewer mustknow.
%   - What is the problem?
%   - Why is it interesting and important?
%   - Why is it hard?why do naive approaches fail?
%   - Why hasn't it been solved before?(Or, what's wrong with previous
%     proposed solutions?)
%   - What are the key components of my approach and results?Also
%     include any specific limitations.
%   - A final paragraph or subsection: “Summary of Contributions”.
%     It should list the major contributions in bullet form,
%     mentioning in which sections they can be found. This material
%     doubles as an outline of the rest of the paper, saving space and
%     eliminating redundancy
% - Unjustified Choices (are bad)
% - Optimal: Does not mean `very good'
% - Proved: Does not mean `demonstrated'
% - Significant: There is a danger of confusing the informal statement
%   and the statistical claim
% - Use all the Space Available
% - Avoid Weak Language: aim, attempt, might, etc.
% - Use the Active Voice
% - ALWAYS put some variance estimate on performance measures (do
%   everything 10 times and give me the variance of whatever you are
%   reporting)

% Figures:
% - Don't cover the data with the labels!
% - Color helps -Direct labeling helps -Meaningful captions help
%
% Common problem with figures:
% 1.Too many patterns on bars
% 2.Use of both different symbols and different lines
% 3.Too many shades of gray on bars
% 4.Lines too thin (or thick)
% 5.Use of three-dimensional bars for only two variables
% 6.Lettering too small and font difficult to read
% 7.Symbols too small or difficult to distinguish
% 8.Redundant title printed on graph
% 9.Use of gray symbols or lines
% 10.Key outside the graph
% 11.Unnecessary numbers in the axis
% 12.Multiple colors map to the same shade of gray
% 13.Unnecessary shading in background
% 14.Using bitmap graphics (instead of vector graphics)
% 15.General carelessness

% ====================================================================

% TODO:
% - clearly point out novelty of this work
% - contribution compared to earlier work

\begin{document}

% Peter 31 Oct: Current title does not really reflect the topic I
% think:
%\title{Exploring approaches to probablistic record linkage using
%        similarity searching}
% Peter 31 Oct: What about
\title{Efficient and Effective Metric Space Indexing for Complete
       Record Linkage
       Al: Exploring approaches to probabilistic record linkage using similarity searching (metric space indexing?) }
%  \thanks{The authors would like to thank the ...|

\author{Submitted for double-blind review}
%\author{Özgür Akgün\inst{1} and Alan Dearle\inst{1} and Graham Kirby\inst{1} Peter Christen\inst{2}

%\institute{School of Computer Science,
%           University of St Andrews, \\
%           St Andrews, Scotland.~
%           Contact: \email{ozgur.akgun@st-andrews.ac.uk}

%\institute{Research School of Computer Science,
%           The Australian National University, \\
%           Canberra, Australia.~
%           Contact: \email{peter.christen@anu.edu.au}


\maketitle

% MAX 200 words
\begin{abstract}
Probabilistic record linkage is the process of identifying records that refer to
the same real-world entities across two or more databases in situations where entity identifiers are unavailable.
It requires  similarities between records to be approximated using common attributes (such as names, addresses, phone numbers) and for pairs of records to be classified as matches or non-matches.
Record linkage is usually done as a two step process;  in the
first step a blocking or indexing is employed to efficiently identify
candidate record pairs which are then compared in more detail and
classified in the second step.

As we show in this paper, even state-of-the-art blocking and
indexing techniques such as locality sensitive hashing, have two
major drawbacks. First, they often remove some true matching record
pairs that have high similarities due to their heuristic nature,
leading to a loss of recall. Second, they include many record pairs
with low similarity leading to high computational requirements.
Combined, this means a reduction of both effectiveness and efficiency.
% * <al@st-andrews.ac.uk> 2017-10-31T08:46:25.069Z:
%
% > efficiency
%
% ^.

We propose an metric indexing approach to \emph{complete} record
linkage, which ensures all record pairs that have a similarity above
a certain similarity threshold are being \textbf{compared}, while no pairs
with a similarity below the threshold are \textbf{compared}. Our approach
leads to a one-step record linkage process which combines indexing
with comparison and classification resulting in improved
effectiveness and efficiency as we experimentally evaluate on several
real-world databases.

\end{abstract}

\keywords Entity resolution; data matching; similarity search;
         blocking.

% ====================================================================

\section{The Plan}

\vspace{5mm}

Plan in a bulleted list:

\begin{itemize}
\item Design: Traditionally it has two stages. A blocking method needs to be developed separately from a similarity method. With similarity search based record linkage, there is only one method, and that is the similarity method.
\item Efficiency: I am not convinced of this, but Peter thinks there may be efficiency benefits. The reasoning behind this is that with blocking-based methods we would have to calculate some sort of a similarity value first. This value will only be used for blocking, and later a similarity value between candidate pairs will need to be calculated again.
\item Completeness: Similarity search based methods will be complete (with respect to the given similarity method) whereas blocking based methods (including LSH) are likely to be incomplete (there will be high-similarity pairs which are not places in the same block)
\item Data sets
\begin{itemize}
\item Kilmarnock Birth-Death
\item Skye Birth-Death
\item CORA
\item North Carolina Voter Registration DB
\end{itemize}
\item The following is a list of all methods we thought we would have to compare.
\begin{itemize}
\item Similarity search (M-Tree, others?)
\item LSH-blocking
\item Traditional blocking (e.g. Region, Surname) - \textbf{why this? - this was done in the paper: A Comparison of Blocking Methods for Record Linkage}
\item MIFile
\item Sorted Neighbourhood blocking - \textbf{why this?}
\end{itemize}
\item Pairs-completeness and pairs-quality (w.r.t high similarity instead of true-link status). I will expand on this a bit more later.
\item Comparison (for blocking based methods)
		Precision/Recall/F1-Measure
\item Which similarity methods are metric?
\item Run time and memory usage comparisons
\item Theorem. Precision might get better or worse with similarity search based methods, but recall should never get worse.
\end{itemize}

Note: The rest of this paper should be regarded as a placeholder for now.

\section{Introduction}
\label{sec-intro}

something about record linkage and its importance, application examples

drawback of existing approaches: blocking and comparisons/classification
done separately, sometimes if bad linkage quality means to go back to redo
blocking.

something about similarity spaces / metric spaces, M-trees and its
more recent advanced/improved versions, and how they have been used in
similarity search.

How our work is different from these other metric space works, and
description of our contributions

The motivation for this work is the Digitising Scotland project (Dibben 2012), which is in the process of transcribing and linking all the vital events recorded in Scotland between 1856 and 1977. This data set will, when complete, include around 14 million birth records, 11 million death records and 4 million marriage records. As part of the work, certain data fields (locations, occupations and causes of death) are also being classified to the relevant standard coding schemes.


% --------------------------------------------------------------------

\section{Related Work}
\label{sec-related}

summarise related work in blocking/indexing

summarise related work in metric spaces as used in record linkage

other related works as relevant

% --------------------------------------------------------------------

\section{Approach}
\label{sec-approach}

In this paper we explore the efficacy of the use of different similarity search algorithms in probabilistic record linkage. We explore two inexact algorithms: LSH and MIFile \cite{amato2014mi} one exact method: mTrees \cite{paolociaccia2m}. We also use a simple brute force exact technique as a baseline for comparison although this approach can only be applied to the smallest of our datasets.

coverage: proportion of all rec pairs with a certain similarity
(independent of their match status) that are being generated by
a blocking  technique

transitive closure is a problem if we have a blocking technique with overlapping blocks.

define blocking: only aimed at improving computational aspects by reducing the number of record pairs to be compared


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

% subsection

%  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

%\subsection

% --------------------------------------------------------------------



%  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\subsection{Analysis and Limitations}
\label{sec-analysis}

maybe a complexity analysis? maybe an analysis of expected 
linkage quality?

% --------------------------------------------------------------------

\section{Experiments and Results}
\label{sec-data}

We now describe the data sets we use in our evaluation and the
experimental setup we employed to evaluate our proposed metric
indexing approach.

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

% Peter 31 Oct: table style follows LNCS style guide

\begin{table}[t]
\caption{Characteristics of data sets used in experiments.
  \emph{Peter, 31 Oct: what else should we show here?}}
 \label{table-datasets}
  \centering
  \begin{scriptsize}
  %\addtolength{\tabcolsep}{-0.5pt}
  \begin{tabular}{ccccc}
  \hline\noalign{\smallskip}
  Data set~ & ~Number of~ & ~Number of true~
   & Attributes used? & ~?? \\
  name(s)  & records   & matching pairs &
    for indexing & ?? \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  NCVR  & ~224,073 / 224,061~ & ~148,036~ & ? & ? \\
  CORA  & 1,295             & title, authors, venue, year ? & ?
    & ? \\
  Isle of Skye & ? & ?
    & ? & ? \\
  Kilmarnock  & ? & ? & ? & ? \\
  \hline
  \end{tabular}
  \end{scriptsize}
\end{table}

% Peter, 31 Oct: To save space, don't use sub-sections but simply
% \textbf{}, such as:
% \smallskip
%\textbf{Data sets}:~

\subsection{Data Sets}
\label{sec-data}

We used four real data sets from three domains in our experiments. The
first is the CORA data set~\footnote{Available from:
\texttt{http://secondstring.sourceforge.net}}, which contains 1,295
(?) records that refer to XX machine learning publications.
% Peter, 31 Oct: not sure about the following - which Cora version do
% you use? ****
 (\url{https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz}) \textbf{ which consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links.}

The second are a pair of data sets based on the North Carolina Voter
Registration (NCVR) data sets\footnote{Available from: \texttt{http://dl.ncsbe.gov/}}, which we collected in June 2014 and
October 2016. Each of these two data sets contain over five million
records of voters and include attributes with their first names,
surnames, and addresses. Ground truth is provided via a \emph{NCID}
identifier which uniquely identifies a voter. In our experiments we
use two randomly selected sub-set of $224,073$ and $224,061$ records,
respectively, where $148,036$ of those refer to voters that occurred
in both original NCVR data sets but had name and/or address changes
over time (thus leading to around $66\%$ matching records).

\emph{Peter, 31 Oct: We need to be careful not to reveal our identities
as the submission is double-blind - so no mention of DS.}

% Peter, 31 Oct: can the following be shortened to 1 paragraph?

We also experiment over two datasets prepared by researchers pursuing previous projects \cite{reid2002} and \cite{reid2006}, which act as pseudo subsets of the Digitising Scotland data set. One data set contains records of vital events registered on the Isle of Skye, a rural district, while the other contains records from Kilmarnock, an industrial town. Both cover the period 1861-1901. These are different types of communities, with different family structures and name distributions \cite{reid2002}. Both datasets are encoded with the same schemata and include names, sex, addresses and the names of each person's father and mother. Ground truth is provided using demographer encoded identifiers which provide (incomplete) linkage between the records.  The Isle of Skye dataset comprises around 17,600 birth records, 12,300 death records, and 2,700 marriage records.
The demographers  identified:
\begin{itemize}
\item 4,300 family groups of siblings, with a mean family size of 3.9 siblings and a maximum family size of 16,
\item 2,900 links from a birth record to the child's death record,
\item 60 links from a marriage record to the groom's death record, and
\item 100 links from a marriage record to the bride's death record.
\end{itemize}
\emph{Peter 31 Oct: what are you actually using in your experiments?
All certificates or only birth?}

The Kilmarnock data set contains around 38,400 birth records, 23,700 death records, and 8,700 marriage records. Here, the demographers identified:
\begin{itemize}
\item 13,100 family groups of siblings, with a mean family size of 2.8 siblings and a  maximum family size of 16,
\item 8,300 links from a birth record to the child's death record,
\item 30 links from a marriage record to the groom's death record, and
\item 50 links from a marriage record to the bride's death record.
\end{itemize}


% --------------------------------------------------------------------

\section{Conclusions and Future Work}
\label{sec-concl}

% --------------------------------------------------------------------

\bibliographystyle{abbrv}
\bibliography{paper.bib} 



\end{document}

% ====================================================================
