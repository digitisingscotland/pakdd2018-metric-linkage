\documentclass{llncs}
%\usepackage{llncsdoc}
\usepackage{epsfig}
\usepackage{graphicx}

%\usepackage{caption}
%\usepackage{subcaption}

\newtheorem{mydef}{Def.}
\newtheorem{myhyp}{Hypothesis}

\usepackage{url}
\usepackage{hyperref}

\pagestyle{empty}

% PAKDD 2018: maximum 12 pages
% Abstract max 200 words
% title, abstract: 1/2 page
% introduction: 1 1/2 page (2 pages so far)
% related work: 1 to 1 1/2 pages (3 pages so far)
% method: 4 pages (7 pages so far)
% experiments: 3 (10 pages so far)
% discussion /conclusion: 1 page
% citations 1

% ====================================================================

% Eamonn ICDM'10 tutorial slides
% - clear problem statement in abstract
% - To convince a reviewer, you must think like a reviewer

% Writing the paper:
% - Make a working title
% - Introduce the topic and define (informally at this stage)
%   terminology
% - Motivation: Emphasize why is the topic important
% - Relate to current knowledge: what’s been done
% - Indicate the gap: what need’s to be done?
% - Formally pose research questions
% - Explain any necessary background material.
% - Introduce formal definitions.
% - Introduce your novel algorithm/representation/data structure etc.
% - Describe experimental set-up, explain what the experiments will
%   show
% - Describe the datasets
% - Summarize results with figures/tables
% - Discuss results
% - Explain conflicting results, unexpected findings and discrepancies
%   with other research
% - State limitations of the study
% - State importance of findings
% - Announce directions for further research
% - Acknowledgements
% - References
%
% - Don’t make the reviewer of your paper think!
% - Reviewers make an initial impression on the first page and don’t
%   change 80% of the time
% - A good introduction with a good motivation is half your success
% - By the end of the introduction the reviewer mustknow.
%   - What is the problem?
%   - Why is it interesting and important?
%   - Why is it hard?why do naive approaches fail?
%   - Why hasn't it been solved before?(Or, what's wrong with previous
%     proposed solutions?)
%   - What are the key components of my approach and results?Also
%     include any specific limitations.
%   - A final paragraph or subsection: “Summary of Contributions”.
%     It should list the major contributions in bullet form,
%     mentioning in which sections they can be found. This material
%     doubles as an outline of the rest of the paper, saving space and
%     eliminating redundancy
% - Unjustified Choices (are bad)
% - Optimal: Does not mean `very good'
% - Proved: Does not mean `demonstrated'
% - Significant: There is a danger of confusing the informal statement
%   and the statistical claim
% - Use all the Space Available
% - Avoid Weak Language: aim, attempt, might, etc.
% - Use the Active Voice
% - ALWAYS put some variance estimate on performance measures (do
%   everything 10 times and give me the variance of whatever you are
%   reporting)

% Figures:
% - Don't cover the data with the labels!
% - Color helps -Direct labeling helps -Meaningful captions help
%
% Common problem with figures:
% 1.Too many patterns on bars
% 2.Use of both different symbols and different lines
% 3.Too many shades of gray on bars
% 4.Lines too thin (or thick)
% 5.Use of three-dimensional bars for only two variables
% 6.Lettering too small and font difficult to read
% 7.Symbols too small or difficult to distinguish
% 8.Redundant title printed on graph
% 9.Use of gray symbols or lines
% 10.Key outside the graph
% 11.Unnecessary numbers in the axis
% 12.Multiple colors map to the same shade of gray
% 13.Unnecessary shading in background
% 14.Using bitmap graphics (instead of vector graphics)
% 15.General carelessness

% ====================================================================

% TODO:
% - clearly point out novelty of this work
% - contribution compared to earlier work

\begin{document}

% Peter 31 Oct: Current title does not really reflect the topic I
% think:
%\title{Exploring approaches to probablistic record linkage using
%        similarity searching}
% Peter 31 Oct: What about
\title{Efficient and Effective Metric Space Indexing for Complete
       Record Linkage
       Al: Exploring approaches to probabilistic record linkage using similarity searching (metric space indexing?) }
%  \thanks{The authors would like to thank the ...|

\author{Submitted for double-blind review}
%\author{Özgür Akgün\inst{1} and Alan Dearle\inst{1} and Graham Kirby\inst{1} Peter Christen\inst{2}

%\institute{School of Computer Science,
%           University of St Andrews, \\
%           St Andrews, Scotland.~
%           Contact: \email{ozgur.akgun@st-andrews.ac.uk}

%\institute{Research School of Computer Science,
%           The Australian National University, \\
%           Canberra, Australia.~
%           Contact: \email{peter.christen@anu.edu.au}


\maketitle

% MAX 200 words
\begin{abstract}
Probabilistic record linkage is the process of identifying records that refer to
the same real-world entities across two or more databases in situations where entity identifiers are unavailable.
It requires  similarities between records to be approximated using common attributes (such as names, addresses, phone numbers) and for pairs of records to be classified as matches or non-matches.
Record linkage is usually performed in a two step process;  in the
first step a blocking or indexing is employed to efficiently group similar
candidate record pairs which are then compared in more detail and
classified in the second step.

As we show in this paper, even state-of-the-art blocking and
indexing techniques such as locality sensitive hashing, have two
major drawbacks. First, they often remove some true matching record
pairs that have high similarities due to their heuristic nature,
leading to a loss of recall. Second, they include many record pairs
with low similarity leading to high computational requirements.
Combined, this means a reduction of both effectiveness and efficiency.
% * <al@st-andrews.ac.uk> 2017-10-31T08:46:25.069Z:
%
% > efficiency
%
% ^.

We propose an metric indexing approach to \emph{complete} record
linkage, which ensures all record pairs that have a similarity above
a certain similarity threshold are being \textbf{compared}, while no pairs
with a similarity below the threshold are \textbf{compared}. Our approach
leads to a one-step record linkage process which combines indexing
with comparison and classification resulting in improved
effectiveness and efficiency as we experimentally evaluate on several
real-world databases.
Parameter space: good linkage bad linkage don't know which without ground truth. M Trees always high no config -> high confidence.

\end{abstract}

\keywords Entity resolution; data matching; similarity search;
         blocking.

% ====================================================================

\section{The Plan}

\vspace{5mm}

Plan in a bulleted list:

\begin{itemize}
\item Design: Traditionally it has two stages. A blocking method needs to be developed separately from a similarity method. With similarity search based record linkage, there is only one method, and that is the similarity method.
\item Efficiency: I am not convinced of this, but Peter thinks there may be efficiency benefits. The reasoning behind this is that with blocking-based methods we would have to calculate some sort of a similarity value first. This value will only be used for blocking, and later a similarity value between candidate pairs will need to be calculated again.
\item Completeness: Similarity search based methods will be complete (with respect to the given similarity method) whereas blocking based methods (including LSH) are likely to be incomplete (there will be high-similarity pairs which are not places in the same block)
\item Data sets
\begin{itemize}
\item Kilmarnock Birth-Death
\item Skye Birth-Death
\item CORA
\item North Carolina Voter Registration DB
\end{itemize}
\item The following is a list of all methods we thought we would have to compare.
\begin{itemize}
\item Similarity search (M-Tree, others?)
\item LSH-blocking
\item Traditional blocking (e.g. Region, Surname) - \textbf{why this? - this was done in the paper: A Comparison of Blocking Methods for Record Linkage}
\item MIFile
\item Sorted Neighbourhood blocking - \textbf{why this?}
\end{itemize}
\item Pairs-completeness and pairs-quality (w.r.t high similarity instead of true-link status). I will expand on this a bit more later.
\item Comparison (for blocking based methods)
		Precision/Recall/F1-Measure
\item Which similarity methods are metric?
\item Run time and memory usage comparisons
\item Theorem. Precision might get better or worse with similarity search based methods, but recall should never get worse.
\end{itemize}

Note: The rest of this paper should be regarded as a placeholder for now.

% --------------------------------------------------------------------

\section{Introduction}
\label{sec-intro}

something about record linkage and its importance, application examples


drawback of existing approaches: blocking and comparisons/classification
done separately, sometimes if bad linkage quality means to go back to redo
blocking.

something about similarity spaces / metric spaces, M-trees and its
more recent advanced/improved versions, and how they have been used in
similarity search.

How our work is different from these other metric space works, and
description of our contributions

The motivation for this work is the Digitising Scotland project (Dibben 2012), which is in the process of transcribing and linking all the vital events recorded in Scotland between 1856 and 1977. This data set will, when complete, include around 14 million birth records, 11 million death records and 4 million marriage records. As part of the work, certain data fields (locations, occupations and causes of death) are also being classified to the relevant standard coding schemes.


% --------------------------------------------------------------------

\section{Related Work}
\label{sec-related}

summarise related work in blocking/indexing

\textbf{Traditional first - Peter?}

Steorts et al. \cite{Steorts2014} examined various approaches to blocking records raging from traditional blocking to variations on Locality Sensitive Hashing (LSH). They assert that traditional blocking based on human selected fields has two major drawbacks. The first is that the blocks that are formed are so large that it remains computationally impractical to compare all the records in the block. Secondly, since similarity is based on a subset of the available fields, records in the same block may differ greatly in the fields that are not considered. \textbf{{this is true of our technique - explore somewhere}}. The paper examines two LSH variations - transitive LSH {ref needed} (TLSH) and K-Means LSH (KLSH). {\textbf{describe these where?}} 
Using simulated data based on people, their data of birth and postal address to evaluate the efficacy of these approaches, they demonstrate that KLSH performs as well or better than commonly used approaches to blocking such as those described \textbf{above}. However, they point out that, like all LSH methods, in order to get good results, it must be tuned to the particular dataset being linked. This in turn requires good ground truth about the dataset which may be unavailable, impossible or very expensive to obtain. \textbf{We examine and address these problems in this paper.}

In \cite{Yu2016} Yu et al. have written an extensive survey on string similarity and join in which the problem is defined as "Given two sets of objects, similarity join aims to find all similar pairs from the two collections". Part of the survey is concerned the  algorithms used to compare two records which are characterised as being character-based, token-based or hybrid. The first, which includes approaches such as Levenshtein {ref needed} use character differences in the strings being compared, the second, which includes techniques such as Jaccard similarity {ref needed}, treats the records as sets of tokens and utilises set similarity, the third hybridises the two approaches. Yu's divides join algorithms (which unify records - also known as linkage) into three categories: a number of techniques based on filtering, verification based and threshold-based. Filtering involves generating signatures for each record and creating inverted lists containing records with common signatures and pruning records with no common signatures. Verification algorithms involve calculating the exact distances of a prefix of the data, and using another technique to estimate the edit distance between records. If the estimated distance is larger than the desired similarity threshold, the candidate may be discarded. The last category (threshold based) includes M-tree, Trie-Join, Pass-Join and Partenum. These approaches all require indexing structures to be constructed that are then used to support comparison.  

{\textbf{do we need to describe these?}}

 \cite{Li2006} describes a method of performing linkage using R-trees {ref needed} that fall into the threshold based category of Yu et al. Their approach demonstrates that high precision and recall may be achieved by measuring Jaccard similarity over selected fields from the records being compared. In their evaluation , their approach achieved a recall of 99 percent in experiments in which conjunctions of similarities over fields were used. In a later paper \cite{Ciaccia97indexingmetric}  Ciaccia et al. demonstrate that using M-trees are almost always more efficient than R-trees.

summarise related work in metric spaces as used in record linkage

other related works as relevant

% --------------------------------------------------------------------

\section{Approach}
\label{sec-approach}

In this paper we explore the efficacy of the use of different similarity search algorithms in probabilistic record linkage. We explore two inexact algorithms: LSH-minhash and MIFile \cite{amato2014mi} one exact method: mTrees \cite{paolociaccia2m}. We also use a simple brute force (nested-loop) exact technique as a baseline for comparison although this approach can only be applied to the smallest of our datasets. All of our experiments have a number of configuration parameters with which we can configure the search space and algorithm behaviours. All have a \textit{distance threshold} below which two records are deemed to represent a link.

In the brute force approach two nested loops are used to compare every record with every other non identical record. The complexity of this approach is n2. If a link can be found with whatever distance metric is used, this approach will find it. Each of our datasets contains ground truth with which we can establish the records that are true links and those that are not. 

The LSH-minhash approach is a fast but inexact method of performing similarity search. It has a number of tuning parameters: \textit{shingle\_size}, the \textit{number\_of\_bands} and the \textit{band\_size}. These are explained in the following algorithm description which follows a standard LSH approach. Firstly all the fields of the record are turned into strings and concatenated. Next the strings are \textit{shingled} into a set of n-grams according to the \textit{shingle\_size} parameter. Next a set of randomly, deterministically generated hash functions are applied to each of the sets of n-grams and the smallest (the minhash) of each application is added to a signature. The number of hashes used and thus the size of the signature is determined by the product of the {number\_of\_bands} and the {band\_size}.  Lastly the signature is divided into {number\_of\_bands} segments and the elements from the band are hashed again to create an key. The original record is added to a hashmap associated with this by this key. It can be shown that the similarities between signatures generated in the second step have very close Jaccard similarity to the original string and that if an arbitrary record is run through this process the records found in the map are close to the record being searched with good precision and recall. We will further explore these properties in this paper.

The MIFile \cite{amato2014mi} approach is another inexact technique for performing similarity search. Using  MI_file,  each record is represented by the ordering of distances from a collection of reference objects. It is based on the idea that if two records are close to each other  will have similar neighbours and will thus be represented by similar of distances from the chosen reference objects. An inverted file is created mapping from... 


In order to compare two objects in the dataset, we compare the two corresponding orderings of the reference objects

{\textbf{need to say something about the operations supported somewhere}}

coverage: proportion of all rec pairs with a certain similarity
(independent of their match status) that are being generated by
a blocking  technique

transitive closure is a problem if we have a blocking technique with overlapping blocks.

define blocking: only aimed at improving computational aspects by reducing the number of record pairs to be compared


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

% subsection

%  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

%\subsection

% --------------------------------------------------------------------



%  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

\subsection{Analysis and Limitations}
\label{sec-analysis}

maybe a complexity analysis? maybe an analysis of expected 
linkage quality?

% --------------------------------------------------------------------

\section{Experiments and Results}
\label{sec-data}

We now describe the data sets we use in our evaluation and the
experimental setup we employed to evaluate our proposed metric
indexing approach.


% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

% Peter 31 Oct: table style follows LNCS style guide

\begin{table}[t]
\caption{Characteristics of data sets used in experiments.
  \emph{Peter, 31 Oct: what else should we show here?}}
 \label{table-datasets}
  \centering
  \begin{scriptsize}
  %\addtolength{\tabcolsep}{-0.5pt}
  \begin{tabular}{ccccc}
  \hline\noalign{\smallskip}
  Data set~ & ~Number of~ & ~Number of true~
   & Attributes used? & ~?? \\
  name(s)  & records   & matching pairs &
    for indexing & ?? \\
  \noalign{\smallskip}
  \hline
  \noalign{\smallskip}
  NCVR  & ~224,073 / 224,061~ & ~148,036~ & ? & ? \\
  CORA  & 1,295             & title, authors, venue, year ? & ?
    & ? \\
  Isle of Skye & ? & ?
    & ? & ? \\
  Kilmarnock  & ? & ? & ? & ? \\
  \hline
  \end{tabular}
  \end{scriptsize}
\end{table}

% Peter, 31 Oct: To save space, don't use sub-sections but simply
% \textbf{}, such as:
% \smallskip
%\textbf{Data sets}:~

\subsection{Data Sets}
\label{sec-data}

We used four real data sets from three domains in our experiments. The
first is the CORA data set~\footnote{Available from:
\texttt{http://secondstring.sourceforge.net}}, which contains 1,295
(?) records that refer to XX machine learning publications.

The second are a pair of data sets based on the North Carolina Voter
Registration (NCVR) data sets\footnote{Available from: \texttt{http://dl.ncsbe.gov/}}, which we collected in June 2014 and
October 2016. Each of these two data sets contain over five million
records of voters and include attributes with their first names,
surnames, and addresses. Ground truth is provided via a \emph{NCID}
identifier which uniquely identifies a voter. In our experiments we
use two randomly selected sub-set of $224,073$ and $224,061$ records,
respectively, where $148,036$ of those refer to voters that occurred
in both original NCVR data sets but had name and/or address changes
over time (thus leading to around $66\%$ matching records).

\emph{Peter, 31 Oct: We need to be careful not to reveal our identities
as the submission is double-blind - so no mention of DS.}
\textbf{I think we need to say something about this to provide context; there is none currently.}

% Peter, 31 Oct: can the following be shortened to 1 paragraph?

We also experiment over two datasets prepared by researchers pursuing previous projects \cite{reid2002} and \cite{reid2006}, which act as pseudo subsets of the Digitising Scotland data set. One data set contains records of vital events registered on the Isle of Skye, a rural district, while the other contains records from Kilmarnock, an industrial town. Both cover the period 1861-1901. These are different types of communities, with different family structures and name distributions \cite{reid2002}. Both datasets are encoded with the same schemata and include names, sex, addresses and the names of each person's father and mother. Ground truth is provided using demographer encoded identifiers which provide (incomplete) linkage between the records.  The Isle of Skye dataset comprises around 17,600 birth records, 12,300 death records, and 2,700 marriage records.
The demographers  identified:
\begin{itemize}
\item 4,300 family groups of siblings, with a mean family size of 3.9 siblings and a maximum family size of 16,
\item 2,900 links from a birth record to the child's death record,
\item 60 links from a marriage record to the groom's death record, and
\item 100 links from a marriage record to the bride's death record.
\end{itemize}
In the experiments described in this paper we examine links between birth and death records since these provided the highest levels of ground truth.

The Kilmarnock data set contains around 38,400 birth records, 23,700 death records, and 8,700 marriage records. Here, the demographers identified:
\begin{itemize}
\item 13,100 family groups of siblings, with a mean family size of 2.8 siblings and a  maximum family size of 16,
\item 8,300 links from a birth record to the child's death record,
\item 30 links from a marriage record to the groom's death record, and
\item 50 links from a marriage record to the bride's death record.
\item 
\end{itemize}
Again only links between birth and death have been used in the experiments in this paper.


% --------------------------------------------------------------------

\section{Conclusions and Future Work}
\label{sec-concl}

% --------------------------------------------------------------------

\bibliographystyle{abbrv}
\bibliography{paper.bib} 



\end{document}

% ====================================================================
